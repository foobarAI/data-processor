{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "import json\n",
    "import EFZP #Package that helps to break down emails into salutations, body, signature, subject lines\n",
    "\n",
    "def normalizetext(text):\n",
    "    #Currently removes newline character and carriage return; converts everything to lowercase\n",
    "#     print repr(text)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    return text.lower()\n",
    "    \n",
    "def splitIntoSentences(text):\n",
    "    text = normalizetext(text)\n",
    "#     print text\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def getMaxWordsBoundedSentences(sentences, nwords):\n",
    "    tcount = 0\n",
    "    i = 0\n",
    "    for s in sentences:\n",
    "        words = word_tokenize(s)\n",
    "        tcount += len(words)\n",
    "        if tcount >= nwords:\n",
    "            break\n",
    "        i += 1\n",
    "    return sentences[:i]\n",
    "\n",
    "def writeTrainableSentencesToFile(filename, sentences):\n",
    "    sentStr = \" \".join(sentences)\n",
    "    sentStr += \"\\n\"\n",
    "    filename.write(sentStr)\n",
    "    return True\n",
    "    \n",
    "# def writeTrainableSubjectLineToFile(filename, sLine):\n",
    "#     pass\n",
    "\n",
    "def createDataSet(emailJsonDoc):\n",
    "#     with open(\"SubjectLines.txt\") as emails:\n",
    "#         emailData = json.load(emails)\n",
    "#         emails.close()\n",
    "#         emailObj = json.loads(emailData)\n",
    "#     print len(emailData[0])\n",
    "    # body = []\n",
    "    # subjectLine = []\n",
    "    # bodyfn = lambda x: x['Body']\n",
    "    # sLinefn = lambda x: x['Subject Line']\n",
    "#     fromFile = open('var/db/from.txt', 'w')\n",
    "#     toFile = open('var/db/to.txt', 'w')\n",
    "    fromFile = open('from.txt', 'w') #body file\n",
    "    toFile = open('to.txt', 'w') #subjectline file\n",
    "#     print (emailJsonDoc[0]['Subject'])\n",
    "    body, subjectLine = map(list, zip(*[(d['body'], d['headers']['Subject']) for d in emailJsonDoc]))\n",
    "    i = 0\n",
    "    for b, s in zip(body, subjectLine):\n",
    "#         i += 1\n",
    "#         if i%7 == 0:\n",
    "#             print repr(EFZP.get_body(str(b)))\n",
    "        bodySentences = splitIntoSentences(EFZP.get_body(str(b)))\n",
    "#             break\n",
    "        trainableBodySentences = getMaxWordsBoundedSentences(bodySentences, 50)\n",
    "        trainableSubjectLines = getMaxWordsBoundedSentences(splitIntoSentences(s), 20)\n",
    "        if trainableBodySentences != [] and trainableSubjectLines != []:\n",
    "#             if i%7 == 0:\n",
    "            writeTrainableSentencesToFile(fromFile, trainableBodySentences)\n",
    "            writeTrainableSentencesToFile(toFile, trainableSubjectLines)\n",
    "#                 writeTrainableBodyToFile(fromFile, trainableBodySentences.extend([\"hello world\"]))\n",
    "#                 writeTrainableSubjectLineToFile(toFile, trainableSubjectLines)\n",
    "#                 break\n",
    "#     pr\n",
    "#     print body[10]\n",
    "#     dataSize = len(body)\n",
    "#     body = map(lambda l: \"\".join(l.split('\\n')) + '\\n', [l for l in body])\n",
    "#     fromFile = open('var/db/from.txt', 'w')\n",
    "#     devFromFile = open('var/db/dev_from.txt', 'w')\n",
    "#     map(lambda s: fromFile.write(s), [l for l in body])\n",
    "#     map(lambda s: devFromFile.write(s), [l for l in body[:int(round(dataSize * 0.4))]])\n",
    "    fromFile.close()\n",
    "#     devFromFile.close()\n",
    "\n",
    "#     subjectLine = map(lambda l: \"\".join(l.split('\\n')) + '\\n', [l for l in subjectLine])\n",
    "#     toFile = open('var/db/to.txt', 'w')\n",
    "#     devToFile = open('var/db/dev_to.txt', 'w')\n",
    "#     map(lambda s: toFile.write(s), [l for l in subjectLine])\n",
    "#     map(lambda s: devToFile.write(s), [l for l in body[:int(round(dataSize * 0.4))]])\n",
    "    toFile.close()\n",
    "#     devToFile.close()\n",
    "# createDataSet()\n",
    "# def test():\n",
    "# #     writeTrainableBodyToFile(None, [\"hello world\", \"how are you doing?\"])\n",
    "#     createDataSet()\n",
    "# test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n",
      "9 7\n",
      "10 8\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "a.extend([9,10])\n",
    "b.extend([7,8])\n",
    "for a,b in zip(a,b):\n",
    "    print a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Folks,\n",
      "?\n",
      "As we always do, here are a few stories from Bob  Harris and the TFL Report.? \n",
      "Great stuff as always.\n",
      "?\n",
      "Enjoy.\n",
      "?\n",
      "Joe\n",
      "? \n",
      "JANIKOWSKI? WHEATLEY? ASK ANYBODY  BUT GRUDEN...\n",
      "[IMAGE]\n",
      "?Written By TFL Report Editor Bob Harris\n",
      "\n",
      " - 0.gif\n",
      "-----------------\n",
      "?\n",
      "As we always do, here are a few stories from Bob  Harris and the TFL Report.? \n",
      "Great stuff as always.\n",
      "?\n",
      "Enjoy.\n",
      "?\n",
      "Joe\n",
      "? \n",
      "JANIKOWSKI? WHEATLEY? ASK ANYBODY  BUT GRUDEN...\n",
      "[IMAGE]\n",
      "?Written By TFL Report Editor Bob Harris\n",
      "\n",
      " - 0.gif\n"
     ]
    }
   ],
   "source": [
    "t1 = db['messages']\n",
    "# print t1.find_one()['headers']['Subject']\n",
    "i = 0\n",
    "emailDocs = []\n",
    "for doc in t1.find():\n",
    "    i += 1\n",
    "    emailDocs.append(doc)\n",
    "#     print doc\n",
    "    if i%36 == 0:\n",
    "        print doc['body']\n",
    "        print \"-----------------\"\n",
    "        print EFZP.get_body(doc['body'])\n",
    "        break\n",
    "# print(len(emailDocs))\n",
    "createDataSet(emailDocs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 8 ms, total: 124 ms\n",
      "Wall time: 126 ms\n",
      "316113\n",
      "CPU times: user 156 ms, sys: 8 ms, total: 164 ms\n",
      "Wall time: 179 ms\n",
      "88225\n"
     ]
    }
   ],
   "source": [
    "with open('to.txt') as to:\n",
    "#     %time data = to.read().replace('\\n', '')\n",
    "#     %time freq = data.count('a')\n",
    "    %time freq1 = sum('a' in l for l in to)\n",
    "    print freq1\n",
    "with open('to.txt') as to:\n",
    "    %time freq2= sum('b' in l for l in to)\n",
    "    print freq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
